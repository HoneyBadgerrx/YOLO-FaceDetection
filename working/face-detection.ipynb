{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-04T22:57:16.609580Z",
     "iopub.status.busy": "2025-02-04T22:57:16.609280Z",
     "iopub.status.idle": "2025-02-04T22:57:19.574936Z",
     "shell.execute_reply": "2025-02-04T22:57:19.574054Z",
     "shell.execute_reply.started": "2025-02-04T22:57:16.609555Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17.1\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf # You know what tensorflow is used for\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, ReLU, SeparableConv2D, AveragePooling2D, Dense, Add\n",
    "from tensorflow.keras.models import Model\n",
    "print(tf.__version__)\n",
    "\n",
    "from pathlib import Path\n",
    "import PIL\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T22:57:32.406717Z",
     "iopub.status.busy": "2025-02-04T22:57:32.406370Z",
     "iopub.status.idle": "2025-02-04T22:57:32.621067Z",
     "shell.execute_reply": "2025-02-04T22:57:32.620133Z",
     "shell.execute_reply.started": "2025-02-04T22:57:32.406690Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "# Make sure TensorFlow uses the GPU\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took  234.8183400630951 seconds to perform the labelling\n"
     ]
    }
   ],
   "source": [
    "#Function to convert labels into my convnet labels\n",
    "conv_output = 20 # A 20x20x5 grid output from the convnet\n",
    "img_size = 224\n",
    "i = 2\n",
    "np.set_printoptions(linewidth=np.inf, precision=10, threshold=np.inf)\n",
    "def set_labels(conv_output):\n",
    "    global i\n",
    "    directory_train = Path(\"../input/face-detection-dataset/labels/train/\")\n",
    "    directory_val = Path(\"../input/face-detection-dataset/labels/val/\")\n",
    "    directory_new_train = Path('./train')\n",
    "    directory_new_val = Path('./val')\n",
    "\n",
    "    # Check if the directory exists, and if not, create it\n",
    "    directory_new_train.mkdir(exist_ok=True)\n",
    "    directory_new_val.mkdir(exist_ok=True)\n",
    "    \n",
    "    #First train directory\n",
    "    for file_path in directory_train.glob('*.txt'):\n",
    "        with file_path.open('r') as file:\n",
    "            content = file.read()\n",
    "            content = content.split('\\n')\n",
    "            content = [s.split(' ') for s in content if s]\n",
    "            content = [[float(i) for i in element] for element in content]\n",
    "            placeholder = np.ones((conv_output,conv_output,5))\n",
    "            for box in content:\n",
    "                bx = math.floor(box[1] * conv_output)\n",
    "                by = math.floor(box[2] * conv_output)\n",
    "                # I know I can do this at once but I like the clarity here\n",
    "                placeholder[by][bx][0] = 0 #set the confidence of the box containing obect midpoint\n",
    "                placeholder[by][bx][1:] = box[1:] #set the bounding box coordinates \n",
    "            placeholder = placeholder.reshape(-1,)\n",
    "            placeholder = ' '.join(map(str, placeholder))\n",
    "            k = directory_new_train / file_path.name.split('/')[-1]\n",
    "            with k.open('w') as file2:\n",
    "                file2.write(placeholder)\n",
    "                \n",
    "    #Second val directory\n",
    "    for file_path in directory_val.glob('*.txt'):\n",
    "        with file_path.open('r') as file:\n",
    "            content = file.read()\n",
    "            content = content.split('\\n')\n",
    "            content = [s.split(' ') for s in content if s]\n",
    "            content = [[float(i) for i in element] for element in content]\n",
    "            placeholder = np.ones((conv_output,conv_output,5))\n",
    "        for box in content:\n",
    "            bx = math.floor(box[1] * conv_output)\n",
    "            by = math.floor(box[2] * conv_output)\n",
    "            # I know I can do this at once but I like the clarity here\n",
    "            placeholder[by][bx][0] = 0 #set the confidence of the box containing obect midpoint\n",
    "            placeholder[by][bx][1:] = box[1:] #set the bounding box coordinates \n",
    "        placeholder = placeholder.reshape(-1,)\n",
    "        placeholder = ' '.join(map(str, placeholder))\n",
    "        k = directory_new_val / file_path.name.split('/')[-1]\n",
    "        with k.open('w') as file2:\n",
    "            file2.write(placeholder)\n",
    "a = time.time()\n",
    "set_labels(conv_output)\n",
    "b = time.time()\n",
    "print(\"It took \",b - a,\"seconds to perform the labelling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# k = tf.io.read_file(tf.constant('./train/90cc772eb8d72107.txt'))\n",
    "# k = tf.strings.split(k, sep=' ')\n",
    "# k = tf.strings.to_number(k, out_type=tf.float32)\n",
    "# k = tf.reshape(k, [20,20,5])\n",
    "# print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T23:04:28.985401Z",
     "iopub.status.busy": "2025-02-04T23:04:28.985058Z",
     "iopub.status.idle": "2025-02-04T23:04:31.718241Z",
     "shell.execute_reply": "2025-02-04T23:04:31.717532Z",
     "shell.execute_reply.started": "2025-02-04T23:04:28.985377Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "conv_output = 20 # A 20x20x5 grid output from the convnet\n",
    "img_size = 224\n",
    "i = 2\n",
    "#The separators for label are important in both functions depending on windows or linux\n",
    "with tf.device('/GPU:0'):\n",
    "    def load_image_and_labels(image_path):\n",
    "        \"\"\"Loads image and labels from a .txt file.\"\"\"\n",
    "        image = tf.io.read_file(image_path)  # Use tf.io for image reading\n",
    "        image = tf.io.decode_jpeg(image, channels=3) # Decode JPEG (adjust if needed)\n",
    "        # Check if the image is grayscale and convert if necessary\n",
    "        if image.shape[-1] == 1: # If it is a gray scale image convert it to RGB\n",
    "            image = tf.image.grayscale_to_rgb(image)\n",
    "        image = tf.image.resize(image, (img_size,img_size),preserve_aspect_ratio=True, antialias=True) # Resize images (important for consistent input)\n",
    "        image = tf.image.resize_with_pad(image, img_size, img_size)\n",
    "        image = tf.cast(image, tf.float32) / 255.0  # Normalize pixel values (0-1)\n",
    "    \n",
    "         # Get label path\n",
    "        #label_path = tf.strings.regex_replace(image_path, 'images', 'labels')\n",
    "        #label_path = tf.strings.regex_replace(label_path, '\\.jpg$', '.txt')\n",
    "        label_path = tf.strings.regex_replace(image_path, '\\.jpg$', '.txt')\n",
    "        str_const = tf.constant('./train/')\n",
    "        label = tf.io.read_file(tf.strings.join([str_const, tf.strings.split(label_path, sep='\\\\')[-1]]))\n",
    "        label = tf.strings.split(label, sep=' ')\n",
    "        label = tf.strings.to_number(label, out_type=tf.float32)\n",
    "        label = tf.reshape(label, [20,20,5])\n",
    "        \n",
    "        # # Read label file\n",
    "        # label = tf.io.read_file(label_path)\n",
    "        # label = tf.strings.split(label, '\\r\\n')\n",
    "    \n",
    "        # # To filter out empty strings\n",
    "        # label = tf.strings.regex_replace(label, '^$', '')\n",
    "        # label = tf.boolean_mask(label, tf.strings.length(label) > 0)\n",
    "        \n",
    "        # # Convert label to tensor (assuming YOLO format: class x_center y_center width height)\n",
    "        # label = tf.strings.strip(label)\n",
    "        # label = tf.strings.split(label, ' ')\n",
    "        # label = tf.strings.to_number(label, out_type=tf.float32)\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "    def create_dataset(image_dir):\n",
    "        \"\"\"Creates a tf.data.Dataset from image and label files.\"\"\"\n",
    "        dataset = tf.data.Dataset.list_files(image_dir, shuffle=False)  # Or *.png, etc.\n",
    "        # image_paths = list(Path(image_dir).glob('*.jpg'))  # Get all image paths\n",
    "        # image_paths = [str(p) for p in image_paths]\n",
    "        #dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n",
    "        dataset = dataset.map(load_image_and_labels,num_parallel_calls=tf.data.AUTOTUNE)  # Parallel processing\n",
    "        return dataset\n",
    "\n",
    "    def load_image_and_labels2(image_path):\n",
    "        \"\"\"Loads image and labels from a .txt file.\"\"\"\n",
    "        image = tf.io.read_file(image_path)  # Use tf.io for image reading\n",
    "        image = tf.io.decode_jpeg(image, channels=3) # Decode JPEG (adjust if needed)\n",
    "        # Check if the image is grayscale and convert if necessary\n",
    "        if image.shape[-1] == 1: # If it is a gray scale image convert it to RGB\n",
    "            image = tf.image.grayscale_to_rgb(image)\n",
    "        image = tf.image.resize(image, (img_size,img_size),preserve_aspect_ratio=True, antialias=True) # Resize images (important for consistent input)\n",
    "        image = tf.image.resize_with_pad(image, img_size, img_size)\n",
    "        image = tf.cast(image, tf.float32) / 255.0  # Normalize pixel values (0-1)\n",
    "    \n",
    "         # Get label path\n",
    "        #label_path = tf.strings.regex_replace(image_path, 'images', 'labels')\n",
    "        #label_path = tf.strings.regex_replace(label_path, '\\.jpg$', '.txt')\n",
    "        label_path = tf.strings.regex_replace(image_path, '\\.jpg$', '.txt')\n",
    "        str_const = tf.constant('./val/')\n",
    "        label = tf.io.read_file(tf.strings.join([str_const, tf.strings.split(label_path, sep='\\\\')[-1]]))\n",
    "        label = tf.strings.split(label, sep=' ')\n",
    "        label = tf.strings.to_number(label, out_type=tf.float32)\n",
    "        label = tf.reshape(label, [20,20,5])\n",
    "        \n",
    "        # # Read label file\n",
    "        # label = tf.io.read_file(label_path)\n",
    "        # label = tf.strings.split(label, '\\r\\n')\n",
    "    \n",
    "        # # To filter out empty strings\n",
    "        # label = tf.strings.regex_replace(label, '^$', '')\n",
    "        # label = tf.boolean_mask(label, tf.strings.length(label) > 0)\n",
    "        \n",
    "        # # Convert label to tensor (assuming YOLO format: class x_center y_center width height)\n",
    "        # label = tf.strings.strip(label)\n",
    "        # label = tf.strings.split(label, ' ')\n",
    "        # label = tf.strings.to_number(label, out_type=tf.float32)\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "    def create_dataset2(image_dir):\n",
    "        \"\"\"Creates a tf.data.Dataset from image and label files.\"\"\"\n",
    "        dataset = tf.data.Dataset.list_files(image_dir, shuffle=False)  # Or *.png, etc.\n",
    "        # image_paths = list(Path(image_dir).glob('*.jpg'))  # Get all image paths\n",
    "        # image_paths = [str(p) for p in image_paths]\n",
    "        #dataset = tf.data.Dataset.from_tensor_slices(image_paths)\n",
    "        dataset = dataset.map(load_image_and_labels2,num_parallel_calls=tf.data.AUTOTUNE)  # Parallel processing\n",
    "        return dataset\n",
    "# Example usage:\n",
    "\n",
    "    image_directory = '../input/face-detection-dataset/images/train/*.jpg'\n",
    "    val_image_directory = '../input/face-detection-dataset/images/val/*.jpg'\n",
    "    dataset = create_dataset(image_directory)\n",
    "    val_dataset = create_dataset2(val_image_directory)\n",
    "    # for image, boxes in dataset.take(5):\n",
    "    #     print(\"Image shape:\", image.shape)\n",
    "    #     tf.print(\"Boxes shape:\", boxes)\n",
    "                \n",
    "    # Batching and other dataset operations:\n",
    "    dataset = dataset.batch(32)\n",
    "    dataset = dataset.shuffle(buffer_size=1000)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    val_dataset = val_dataset.batch(32)\n",
    "    val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T23:04:32.433991Z",
     "iopub.status.busy": "2025-02-04T23:04:32.433516Z",
     "iopub.status.idle": "2025-02-04T23:05:54.695434Z",
     "shell.execute_reply": "2025-02-04T23:05:54.694343Z",
     "shell.execute_reply.started": "2025-02-04T23:04:32.433952Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (32, 224, 224, 3)\n",
      "Boxes shape: (32, 20, 20, 5)\n",
      "Image shape: (32, 224, 224, 3)\n",
      "Boxes shape: (32, 20, 20, 5)\n"
     ]
    }
   ],
   "source": [
    "for image, label in dataset.take(1):\n",
    "    print(\"Image shape:\", image.shape)\n",
    "    print(\"Boxes shape:\", label.shape)\n",
    "\n",
    "for image, label in val_dataset.take(1):\n",
    "    print(\"Image shape:\", image.shape)\n",
    "    print(\"Boxes shape:\", label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..\\input\\face-detection-dataset\\images\\train\n",
      "13386\n"
     ]
    }
   ],
   "source": [
    "\n",
    "image_directory = Path('../input/face-detection-dataset/images/train')\n",
    "print(image_directory)\n",
    "image_count = len(list(image_directory.glob('*.jpg')))\n",
    "print(image_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "images = image_directory.glob('*.jpg')\n",
    "PIL.Image.open(str(list(images)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T23:08:41.684308Z",
     "iopub.status.busy": "2025-02-04T23:08:41.683978Z",
     "iopub.status.idle": "2025-02-04T23:08:41.788351Z",
     "shell.execute_reply": "2025-02-04T23:08:41.787706Z",
     "shell.execute_reply.started": "2025-02-04T23:08:41.684279Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):  \n",
    "    inputs = tf.keras.Input(shape=(img_size,img_size,3))\n",
    "    x = Conv2D(filters=32, kernel_size=3, padding='same')(inputs) #outputs 224x224x32\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU(negative_slope=0.01)(x)\n",
    "    '''START OF BOTTLENECK'''\n",
    "    input_to_bottleneck = x\n",
    "    x = Conv2D(6 * 32, 1)(x) #outputs 224x224x192 expansion\n",
    "    x = SeparableConv2D(32, 3,padding='same')(x) #outputs 224x224x32 depthwise separable\n",
    "    x = Add()([input_to_bottleneck, x])\n",
    "    '''END OF BOTTLENECK'''\n",
    "    x = AveragePooling2D(2,strides=2)(x) #Outputs 112x112x32\n",
    "    '''START OF BOTTLENECK'''\n",
    "    input_to_bottleneck = x\n",
    "    x = Conv2D(6 * 32 * 3, 1)(x) #outputs 112x112x576 expansion\n",
    "    x = SeparableConv2D(32, 3,padding='same')(x) #outputs 112x112x32 depthwise separable\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU(negative_slope=0.01)(x)\n",
    "    x = Add()([input_to_bottleneck, x])\n",
    "    '''END OF BOTTLENECK'''\n",
    "    x = AveragePooling2D(2,strides=2)(x) #Outputs 56x56x32\n",
    "    '''START OF BOTTLENECK'''\n",
    "    input_to_bottleneck = x\n",
    "    x = Conv2D(6 * 32 * 3 * 2, 1)(x) #outputs 56x56x1152 expansion\n",
    "    x = SeparableConv2D(32, 3,padding='same')(x) #outputs 56x56x32 depthwise separable\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU(negative_slope=0.01)(x)\n",
    "    x = Add()([input_to_bottleneck, x])\n",
    "    '''END OF BOTTLENECK'''\n",
    "    x = AveragePooling2D(2,strides=2)(x) #Outputs 28x28x32\n",
    "    '''START OF BOTTLENECK'''\n",
    "    input_to_bottleneck = x\n",
    "    x = Conv2D(6 * 32 * 3, 1)(x) #outputs 28x28x576 expansion\n",
    "    x = SeparableConv2D(32, 3,padding='same')(x) #outputs 28x28x32 depthwise separable\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU(negative_slope=0.01)(x)\n",
    "    x = Add()([input_to_bottleneck, x])\n",
    "    '''END OF BOTTLENECK'''\n",
    "    x = SeparableConv2D(16, 5,padding='valid')(x) #outputs 24x24x16 depthwise separable\n",
    "    outputs = SeparableConv2D(5, 5,padding='valid')(x) #outputs 20x20x5 depthwise separable\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs,outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T23:08:28.698206Z",
     "iopub.status.busy": "2025-02-04T23:08:28.697905Z",
     "iopub.status.idle": "2025-02-04T23:08:28.704071Z",
     "shell.execute_reply": "2025-02-04T23:08:28.703179Z",
     "shell.execute_reply.started": "2025-02-04T23:08:28.698185Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def decoder(labels, batch_shape):\n",
    "#     placeholder = tf.ones([batch_shape,20,20,5])\n",
    "#     for example in range(len(labels)):\n",
    "#         for box in range(len(example)):\n",
    "#             bx = math.floor(labels[example][box][1] * 224)\n",
    "#             by = math.floor(labels[example][box][2] * 224)\n",
    "#             placeholder[example][by][bx][0] = 0 #set the confidence of the box containing obect midpoint\n",
    "#             placeholder[example][by][bx][1:] = box[1:] #set the bounding box coordinates\n",
    "#     return placeholder\n",
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "import keras\n",
    "@keras.saving.register_keras_serializable()\n",
    "def customLoss(y_true,y_pred):\n",
    "    \"\"\"\n",
    "    Custom loss for Yolo detection. Speciliased for face detection dataset\n",
    "    \"\"\"\n",
    "    loss_bounding_box = 5\n",
    "    loss_noobj = 0.5\n",
    "\n",
    "    #Decode the tensors\n",
    "    #new_y_true = decoder(y_true, y_true.shape[0])\n",
    "    true_confidence = y_true[...,0]\n",
    "    true_coordinates = y_true[...,1:]\n",
    "    pred_confidence = y_pred[...,0]\n",
    "    pred_coordinates = y_pred[...,1:]\n",
    "\n",
    "    #mask for obj present and noobj present in box\n",
    "    mask_obj = true_confidence == 0\n",
    "    mask_noobj = true_confidence == 1\n",
    "\n",
    "    # Confidence loss\n",
    "    obj_loss = bce(true_confidence[mask_obj], pred_confidence[mask_obj])\n",
    "    no_obj_loss = bce(true_confidence[mask_noobj], pred_confidence[mask_noobj])\n",
    "    confidence_loss = obj_loss + (loss_noobj * no_obj_loss)\n",
    "\n",
    "    #bounding box loss\n",
    "    box_loss = mse(true_coordinates[mask_obj], pred_coordinates[mask_obj])\n",
    "    box_loss *= loss_bounding_box\n",
    "\n",
    "    #total yolo loss\n",
    "    total_loss = confidence_loss + box_loss\n",
    "\n",
    "    return total_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T23:08:46.278607Z",
     "iopub.status.busy": "2025-02-04T23:08:46.278303Z",
     "iopub.status.idle": "2025-02-04T23:08:46.288211Z",
     "shell.execute_reply": "2025-02-04T23:08:46.287428Z",
     "shell.execute_reply.started": "2025-02-04T23:08:46.278584Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#tf.config.run_functions_eagerly(False)\n",
    "model.compile(loss=customLoss,optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T23:08:55.428113Z",
     "iopub.status.busy": "2025-02-04T23:08:55.427806Z",
     "iopub.status.idle": "2025-02-04T23:12:23.735879Z",
     "shell.execute_reply": "2025-02-04T23:12:23.734900Z",
     "shell.execute_reply.started": "2025-02-04T23:08:55.428092Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.fit(dataset, validation_data=val_dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T23:12:23.737480Z",
     "iopub.status.busy": "2025-02-04T23:12:23.737141Z",
     "iopub.status.idle": "2025-02-04T23:12:23.968123Z",
     "shell.execute_reply": "2025-02-04T23:12:23.967448Z",
     "shell.execute_reply.started": "2025-02-04T23:12:23.737446Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model('yolo_face.keras', custom_objects={'customLoss': customLoss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T01:23:22.883350Z",
     "iopub.status.busy": "2025-02-05T01:23:22.883031Z",
     "iopub.status.idle": "2025-02-05T01:27:11.346902Z",
     "shell.execute_reply": "2025-02-05T01:27:11.345354Z",
     "shell.execute_reply.started": "2025-02-05T01:23:22.883324Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m419/419\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 284ms/step - loss: 0.5556 - val_loss: 0.6661\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-8a00f375fc7b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pythonify_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1553\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loaded_model.fit(dataset, validation_data=val_dataset, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T01:23:05.927817Z",
     "iopub.status.busy": "2025-02-05T01:23:05.927476Z",
     "iopub.status.idle": "2025-02-05T01:23:05.936416Z",
     "shell.execute_reply": "2025-02-05T01:23:05.935532Z",
     "shell.execute_reply.started": "2025-02-05T01:23:05.927785Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Fine tuning\n",
    "loaded_model.compile(loss=customLoss, optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4, weight_decay=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T01:27:13.439820Z",
     "iopub.status.busy": "2025-02-05T01:27:13.439472Z",
     "iopub.status.idle": "2025-02-05T01:27:13.537710Z",
     "shell.execute_reply": "2025-02-05T01:27:13.537030Z",
     "shell.execute_reply.started": "2025-02-05T01:27:13.439792Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "loaded_model.save('final_yolo.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"PART FOR FILTERING THE OUTPUT OF THE CONVNET\"\"\"\n",
    "#Pick a random image\n",
    "#resize the image\n",
    "#run prediction\n",
    "#clean the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_model = tf.keras.models.load_model('final_yolo.keras', custom_objects={'customLoss': customLoss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 59s/step\n",
      "(32, 20, 20, 5)\n"
     ]
    }
   ],
   "source": [
    "for image,label in val_dataset.take(1):\n",
    "    rand_image = image\n",
    "    print(latest_model.predict(rand_image).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 3345370,
     "sourceId": 5891144,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "mymain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
